{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e706176ec88dc9",
   "metadata": {},
   "source": [
    "### RAG\n",
    "检索增强生成(Retrieval-Augmented Generation, RAG)和Agent应该是目前LLM最火的两个方向，当然这两个方向也不是完全独立的，他们各自能解决各自的问题。我们先来聊聊RAG，引申出langchain中和RAG相关的六个组件。\n",
    "\n",
    "先从一个问题开始：假设我们有一些特别的知识，比如我们室组内部有一个上线的流程规范，这个流程规范可能在文档中以文字，表格，甚至图片的形式存在着的。但是如果我去问一个模型，和这个流程规范相关的任何问题，它要么告诉你不知道，要么就胡说八道，不可能给我们正确的答案。这时候我们可以构造如下的提示词给到模型，那么凭借语言模型的强大泛化能力就可以回答问题：\n",
    "```\n",
    "请根据以下文本回答反引号`中的问题:\n",
    "XXXXX(这里是室组上线流程的具体规范文字)\n",
    "`室组上线之前需要经过哪些步骤，哪些需要通过组长审核`\n",
    "```\n",
    "这其实就是RAG的核心。但是RAG虽然看起来简单，其实细节非常多，虽然我们上面看所谓RAG不过就是把知识当做提示词一部分给到模型然后提问吗。但是我们需要考虑的问题有很多：\n",
    "1. 知识是以各种形式存在的，有些是文档，有些是表格，有些是图片，甚至有些是网页，那么如果给这些知识一个统一的抽象\n",
    "2. 统一抽象的这些知识以什么形式存在哪里\n",
    "3. 我们知道模型窗口是有上下文大小限制的，那么我们就不可能说把所有的知识全部塞给模型，这样既不高效，也不现实，因此我们只能从里面选择一部分和问题相关的片段出来。那么我们如何将知识划分片段又如何去选择哪些片段是和问题相关的\n",
    "4. 得到相关的片段之后，我们又该如何组合这些文档片段，成为一个完整的文字交给模型\n",
    "\n",
    "这些问题其实就是langchain和RAG相关模块帮我们做的：Document Loader，Text Splitters，Embedding Models，Vector Stores和Retrievers。我们来一个个看看。\n",
    "#### Document Loader\n",
    "首先langchain将所有支持抽象成了Document对象，里面是一个带有内容的包装类，同时langchain提供了各种各样的加载器，我们不可能一个个全部过一遍，所以你可以[自己去看看](https://python.langchain.com/v0.1/docs/integrations/document_loaders/)，我们这里介绍一个稍微简单点的Loader:WebBaseLoader。\n",
    "假设我们的问题就是询问和langchain相关的问题，我们先来看看直接询问会是什么样的结果"
   ]
  },
  {
   "cell_type": "code",
   "id": "81961b73a74ffc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:56:27.918111Z",
     "start_time": "2024-06-26T04:56:25.405106Z"
    }
   },
   "source": [
    "from libs.llm.qwen import qwen\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template('什么是langchain')\n",
    "])\n",
    "chain = prompt_template | qwen | StrOutputParser()\n",
    "chain.invoke({})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'抱歉，我并不了解\"langchain\"这个术语。可能您想要询问的是“language chain”或者“LangChain”，但是没有足够的信息让我给出准确的定义。如果您的意思是与自然语言处理（NLP）或区块链技术相关的概念，请提供更多的上下文，我会尽力为您提供帮助。'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "e71657072c767d81",
   "metadata": {},
   "source": [
    "可以看出，对于模型来说，它并不懂什么是langchain，这可能是因为模型训练的时候langchain还不存在，或者没有使用到任何和langchain相关的数据去进行训练。我们通过RAG的手段给它一段外挂知识来将它一步步优化。首先就是langchain的知识去哪里找，当然是langchain的官方文档，也就是我们需要一个能直接从网页读取数据的loader，而WebBaseLoader就是这么一个Loader。"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5bb347268b2c926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:56:29.047671Z",
     "start_time": "2024-06-26T04:56:27.920479Z"
    }
   },
   "source": [
    "# WebBaseLoader.ts\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path='https://python.langchain.com/v0.2/docs/concepts/',\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "        class_=\"docItemContainer_Djhp\"\n",
    "    )),\n",
    "    proxies={'http': 'http://localhost:7890', 'https': 'http://localhost:7890'}\n",
    ")\n",
    "doc = loader.load()\n",
    "print(doc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Conceptual guideOn this pageConceptual guideThis section contains introductions to key parts of LangChain.Architecture\\u200bLangChain as a framework consists of a number of packages.langchain-core\\u200bThis package contains base abstractions of different components and ways to compose them together.\\nThe interfaces for core components like LLMs, vector stores, retrievers and more are defined here.\\nNo third party integrations are defined here.\\nThe dependencies are kept purposefully very lightweight.Partner packages\\u200bWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g. langchain-openai, langchain-anthropic, etc).\\nThis was done in order to improve support for these important integrations.langchain\\u200bThe main langchain package contains chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\nThese are NOT third party integrations.\\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.langchain-community\\u200bThis package contains third party integrations that are maintained by the LangChain community.\\nKey partner packages are separated out (see below).\\nThis contains all integrations for various components (LLMs, vector stores, retrievers).\\nAll dependencies in this package are optional to keep the package as lightweight as possible.langgraph\\u200blanggraph is an extension of langchain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.langserve\\u200bA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.LangSmith\\u200bA developer platform that lets you debug, test, evaluate, and monitor LLM applications.LangChain Expression Language (LCEL)\\u200bLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Runnable interface\\u200bTo make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about below.This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\\nThe standard interface includes:stream: stream back chunks of the responseinvoke: call the chain on an inputbatch: call the chain on a list of inputsThese also have corresponding async methods that should be used with asyncio await syntax for concurrency:astream: stream back chunks of the response asyncainvoke: call the chain on an input asyncabatch: call the chain on a list of inputs asyncastream_log: stream back intermediate steps as they happen, in addition to the final responseastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)The input type and output type varies by component:ComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the toolAll runnables expose input and output schemas to inspect the inputs and outputs:input_schema: an input Pydantic model auto-generated from the structure of the Runnableoutput_schema: an output Pydantic model auto-generated from the structure of the RunnableComponents\\u200bLangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\\nSome components LangChain implements, some components we rely on third-party integrations for, and others are a mix.Chat models\\u200bLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\\nThese are traditionally newer models (older models are generally LLMs, see below).\\nChat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input. This means you can easily use chat models in place of LLMs.When a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.LangChain does not host any Chat Models, rather we rely on third party integrations.We have some standardized parameters when constructing ChatModels:model: the name of the modeltemperature: the sampling temperaturetimeout: request timeoutmax_tokens: max tokens to generatestop: default stop sequencesmax_retries: max number of times to retry requestsapi_key: API key for the model providerbase_url: endpoint to send requests toSome important things to note:standard params only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can\\'t be supported on these.standard params are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they\\'re not enforced on models in langchain-community.ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model.infoTool Calling Some chat models have been fine-tuned for tool calling and provide a dedicated API for tool calling.\\nGenerally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\\nPlease see the tool calling section for more information.For specifics on how to use chat models, see the relevant how-to guides here.Multimodality\\u200bSome chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning model providers haven\\'t standardized on the \"best\" way to define the API. Multimodal outputs are even less common. As such, we\\'ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI\\'s content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.For specifics on how to use multimodal models, see the relevant how-to guides here.For a full list of LangChain model providers with multimodal models, check out this table.LLMs\\u200bcautionPure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as chat completion models,\\neven for non-chat use cases.You are probably looking for the section above instead.Language models that takes a string as input and returns a string.\\nThese are traditionally older models (newer models generally are Chat Models, see above).Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\\nThis gives them the same interface as Chat Models.\\nWhen messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.LangChain does not host any LLMs, rather we rely on third party integrations.For specifics on how to use LLMs, see the relevant how-to guides here.Messages\\u200bSome language models take a list of messages as input and return a message.\\nThere are a few different types of messages.\\nAll messages have a role, content, and response_metadata property.The role describes WHO is saying the message.\\nLangChain has different message classes for different roles.The content property describes the content of the message.\\nThis can be a few different things:A string (most models deal this type of content)A List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and that input location)HumanMessage\\u200bThis represents a message from the user.AIMessage\\u200bThis represents a message from the model. In addition to the content property, these messages also have:response_metadataThe response_metadata property contains additional metadata about the response. The data here is often specific to each model provider.\\nThis is where information like log-probs and token usage may be stored.tool_callsThese represent a decision from an language model to call a tool. They are included as part of an AIMessage output.\\nThey can be accessed from there with the .tool_calls property.This property returns a list of dictionaries. Each dictionary has the following keys:name: The name of the tool that should be called.args: The arguments to that tool.id: The id of that tool call.SystemMessage\\u200bThis represents a system message, which tells the model how to behave. Not every model provider supports this.FunctionMessage\\u200bThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.ToolMessage\\u200bThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI\\'s function and tool message types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to the tool that was called to produce this result.Prompt templates\\u200bPrompt templates help to translate user input and parameters into instructions for a language model.\\nThis can be used to guide a model\\'s response, helping it understand the context and generate relevant and coherent language-based output.Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.There are a few different types of prompt templates:String PromptTemplates\\u200bThese prompt templates are used to format a single string, and generally are used for simpler inputs.\\nFor example, a common way to construct and use a PromptTemplate is as follows:from langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplateChatPromptTemplates\\u200bThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:from langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplateIn the above example, this ChatPromptTemplate will construct two messages when called.\\nThe first is a system message, that has no variables to format.\\nThe second is a HumanMessage, and will be formatted by the topic variable the user passes in.MessagesPlaceholder\\u200bThis prompt template is responsible for adding a list of messages in a particular place.\\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\\nThis is how you use MessagesPlaceholder.from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessageThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\\nThis is useful for letting a list of messages be slotted into a particular spot.An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:prompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])For specifics on how to use prompt templates, see the relevant how-to guides here.Example selectors\\u200bOne common prompting technique for achieving better performance is to include examples as part of the prompt.\\nThis gives the language model concrete examples of how it should behave.\\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\\nExample Selectors are classes responsible for selecting and then formatting examples into prompts.For specifics on how to use example selectors, see the relevant how-to guides here.Output parsers\\u200bnoteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\nMore and more models are supporting function (or tool) calling, which handles this automatically.\\nIt is recommended to use function/tool calling rather than output parsing.\\nSee documentation for that here.Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:Name: The name of the output parserSupports Streaming: Whether the output parser supports streaming.Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.Input Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.Output Type: The output type of the object returned by the parser.Description: Our commentary on this output parser and when to use it.NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionJSON✅✅str | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML✅✅str | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic\\'s).CSV✅✅str | MessageList[str]Returns a list of comma separated values.OutputFixing✅str | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError✅str | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame✅str | MessagedictUseful for doing operations with pandas DataFrames.Enum✅str | MessageEnumParses response into one of the provided enum values.Datetime✅str | Messagedatetime.datetimeParses response into a datetime string.Structured✅str | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.For specifics on how to use output parsers, see the relevant how-to guides here.Chat history\\u200bMost LLM applications have a conversational interface.\\nAn essential component of a conversation is being able to refer to information introduced earlier in the conversation.\\nAt bare minimum, a conversational system should be able to access some window of past messages directly.The concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain.\\nThis ChatHistory will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database.\\nFuture interactions will then load those messages and pass them into the chain as part of the input.Documents\\u200bA Document object in LangChain contains information about some data. It has two attributes:page_content: str: The content of this document. Currently is only a string.metadata: dict: Arbitrary metadata associated with this document. Can track the document id, file name, etc.Document loaders\\u200bThese classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\\nAn example use case is as follows:from langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(    ...  # <-- Integration specific parameters here)data = loader.load()API Reference:CSVLoaderFor specifics on how to use document loaders, see the relevant how-to guides here.Text splitters\\u200bOnce you\\'ve loaded documents, you\\'ll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model\\'s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.At a high level, text splitters work as following:Split the text up into small, semantically meaningful chunks (often sentences).Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).That means there are two different axes along which you can customize your text splitter:How the text is splitHow the chunk size is measuredFor specifics on how to use text splitters, see the relevant how-to guides here.Embedding models\\u200bEmbedding models create a vector representation of a piece of text. You can think of a vector as an array of numbers that captures the semantic meaning of the text.\\nBy representing the text in this way, you can perform mathematical operations that allow you to do things like search for other pieces of text that are most similar in meaning.\\nThese natural language search capabilities underpin many types of context retrieval,\\nwhere we provide an LLM with the relevant data it needs to effectively respond to a query.The Embeddings class is a class designed for interfacing with text embedding models. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).For specifics on how to use embedding models, see the relevant how-to guides here.Vector stores\\u200bOne of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors,\\nand then at query time to embed the unstructured query and retrieve the embedding vectors that are \\'most similar\\' to the embedded query.\\nA vector store takes care of storing embedded data and performing vector search for you.Most vector stores can also store metadata about embedded vectors and support filtering on that metadata before\\nsimilarity search, allowing you more control over returned documents.Vector stores can be converted to the retriever interface by doing:vectorstore = MyVectorStore()retriever = vectorstore.as_retriever()For specifics on how to use vector stores, see the relevant how-to guides here.Retrievers\\u200bA retriever is an interface that returns documents given an unstructured query.\\nIt is more general than a vector store.\\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\\nRetrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.Retrievers accept a string query as input and return a list of Document\\'s as output.For specifics on how to use retrievers, see the relevant how-to guides here.Tools\\u200bTools are interfaces that an agent, a chain, or a chat model / LLM can use to interact with the world.A tool consists of the following components:The name of the toolA description of what the tool doesJSON schema of what the inputs to the tool areThe function to callWhether the result of a tool should be returned directly to the user (only relevant for agents)The name, description and JSON schema are provided as context\\nto the LLM, allowing the LLM to determine how to use the tool\\nappropriately.Given a list of available tools and a prompt, an LLM can request\\nthat one or more tools be invoked with appropriate arguments.Generally, when designing tools to be used by a chat model or LLM, it is important to keep in mind the following:Chat models that have been fine-tuned for tool calling will be better at tool calling than non-fine-tuned models.Non fine-tuned models may not be able to use tools at all, especially if the tools are complex or require multiple tool calls.Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas.Simpler tools are generally easier for models to use than more complex tools.For specifics on how to use tools, see the relevant how-to guides here.Toolkits\\u200bToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.All Toolkits expose a get_tools method which returns a list of tools.\\nYou can therefore do:# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()Agents\\u200bBy themselves, language models can\\'t take actions - they just output text.\\nA big use case for LangChain is creating agents.\\nAgents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.\\nThe results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.\\nPlease check out that documentation for a more in depth overview of agent concepts.There is a legacy agent concept in LangChain that we are moving towards deprecating: AgentExecutor.\\nAgentExecutor was essentially a runtime for agents.\\nIt was a great place to get started, however, it was not flexible enough as you started to have more customized agents.\\nIn order to solve that we built LangGraph to be this flexible, highly-controllable runtime.If you are still using AgentExecutor, do not fear: we still have a guide on how to use AgentExecutor.\\nIt is recommended, however, that you start to transition to LangGraph.\\nIn order to assist in this we have put together a transition guide on how to do so.Callbacks\\u200bLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.Callback Events\\u200bEventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retryCallback handlers\\u200bCallback handlers can either be sync or async:Sync callback handlers implement the BaseCallbackHandler interface.Async callback handlers implement the AsyncCallbackHandler interface.During run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.Passing callbacks\\u200bThe callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:The callbacks are available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:Request time callbacks: Passed at the time of the request in addition to the input data.\\nAvailable on all standard Runnable objects. These callbacks are INHERITED by all children\\nof the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).Constructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks\\nare passed as arguments to the constructor of the object. The callbacks are scoped\\nonly to the object they are defined on, and are not inherited by any children of the object.dangerConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\\nof the object.If you\\'re creating a custom chain or runnable, you need to remember to propagate request time\\ncallbacks to any child objects.Async in Python<=3.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\\nand is running async in python<=3.10, will have to propagate callbacks to child\\nobjects manually. This is because LangChain cannot automatically propagate\\ncallbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\\nrunnables or tools.For specifics on how to use callbacks, see the relevant how-to guides here.Techniques\\u200bStreaming\\u200bIndividual LLM calls often run for much longer than traditional resource requests.\\nThis compounds when you build more complex chains or agents that require multiple reasoning steps.Fortunately, LLMs generate output iteratively, which means it\\'s possible to show sensible intermediate results\\nbefore the final response is ready. Consuming output as soon as it becomes available has therefore become a vital part of the UX\\naround building apps with LLMs to help alleviate latency issues, and LangChain aims to have first-class support for streaming.Below, we\\'ll discuss some concepts and considerations around streaming in LangChain..stream() and .astream()\\u200bMost modules in LangChain include the .stream() method (and the equivalent .astream() method for async environments) as an ergonomic streaming interface.\\n.stream() returns an iterator, which you can consume with a simple for loop. Here\\'s an example with a chat model:from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")for chunk in model.stream(\"what color is the sky?\"):    print(chunk.content, end=\"|\", flush=True)API Reference:ChatAnthropicFor models (or other components) that don\\'t support streaming natively, this iterator would just yield a single chunk, but\\nyou could still use the same general pattern when calling them. Using .stream() will also automatically call the model in streaming mode\\nwithout the need to provide additional config.The type of each outputted chunk depends on the type of component - for example, chat models yield AIMessageChunks.\\nBecause this method is part of LangChain Expression Language,\\nyou can handle formatting differences from different outputs using an output parser to transform\\neach yielded chunk.You can check out this guide for more detail on how to use .stream()..astream_events()\\u200bWhile the .stream() method is intuitive, it can only return the final generated value of your chain. This is fine for single LLM calls,\\nbut as you build more complex chains of several LLM calls together, you may want to use the intermediate values of\\nthe chain alongside the final output - for example, returning sources alongside the final generation when building a chat\\nover documents app.There are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate\\nvalues to the end with something like chained .assign() calls, but LangChain also includes an\\n.astream_events() method that combines the flexibility of callbacks with the ergonomics of .stream(). When called, it returns an iterator\\nwhich yields various types of events that you can filter and process according\\nto the needs of your project.Here\\'s one small example that prints just events containing streamed chat model output:from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for event in chain.astream_events({\"topic\": \"parrot\"}, version=\"v2\"):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event, end=\"|\", flush=True)API Reference:StrOutputParser | ChatPromptTemplate | ChatAnthropicYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!See this guide for more detailed information on how to use .astream_events(),\\nincluding a table listing available events.Callbacks\\u200bThe lowest level way to stream outputs from LLMs in LangChain is via the callbacks system. You can pass a\\ncallback handler that handles the on_llm_new_token event into LangChain components. When that component is invoked, any\\nLLM or chat model contained in the component calls\\nthe callback with the generated token. Within the callback, you could pipe the tokens into some other destination, e.g. a HTTP response.\\nYou can also handle the on_llm_end event to perform any necessary cleanup.You can see this how-to section for more specifics on using callbacks.Callbacks were the first technique for streaming introduced in LangChain. While powerful and generalizable,\\nthey can be unwieldy for developers. For example:You need to explicitly initialize and manage some aggregator or other stream to collect results.The execution order isn\\'t explicitly guaranteed, and you could theoretically have a callback run after the .invoke() method finishes.Providers would often make you pass an additional parameter to stream outputs instead of returning them all at once.You would often ignore the result of the actual model call in favor of callback results.Tokens\\u200bThe unit that most model providers use to measure input and output is via a unit called a token.\\nTokens are the basic units that language models read and generate when processing or producing text.\\nThe exact definition of a token can vary depending on the specific way the model was trained -\\nfor instance, in English, a token could be a single word like \"apple\", or a part of a word like \"app\".When you send a model a prompt, the words and characters in the prompt are encoded into tokens using a tokenizer.\\nThe model then streams back generated output tokens, which the tokenizer decodes into human-readable text.\\nThe below example shows how OpenAI models tokenize LangChain is cool!:You can see that it gets split into 5 different tokens, and that the boundaries between tokens are not exactly the same as word boundaries.The reason language models use tokens rather than something more immediately intuitive like \"characters\"\\nhas to do with how they process and understand text. At a high-level, language models iteratively predict their next generated output based on\\nthe initial input and their previous generations. Training the model using tokens language models to handle linguistic\\nunits (like words or subwords) that carry meaning, rather than individual characters, which makes it easier for the model\\nto learn and understand the structure of the language, including grammar and context.\\nFurthermore, using tokens can also improve efficiency, since the model processes fewer units of text compared to character-level processing.Structured output\\u200bLLMs are capable of generating arbitrary text. This enables the model to respond appropriately to a wide\\nrange of inputs, but for some use-cases, it can be useful to constrain the LLM\\'s output\\nto a specific format or structure. This is referred to as structured output.For example, if the output is to be stored in a relational database,\\nit is much easier if the model generates output that adheres to a defined schema or format.\\nExtracting specific information from unstructured text is another\\ncase where this is particularly useful. Most commonly, the output format will be JSON,\\nthough other formats such as YAML can be useful too. Below, we\\'ll discuss\\na few ways to get structured output from models in LangChain..with_structured_output()\\u200bFor convenience, some LangChain chat models support a .with_structured_output() method.\\nThis method only requires a schema as input, and returns a dict or Pydantic object.\\nGenerally, this method is only present on models that support one of the more advanced methods described below,\\nand will use one of them under the hood. It takes care of importing a suitable output parser and\\nformatting the schema in the right format for the model.For more information, check out this how-to guide.Raw prompting\\u200bThe most intuitive way to get a model to structure output is to ask nicely.\\nIn addition to your query, you can give instructions describing what kind of output you\\'d like, then\\nparse the output using an output parser to convert the raw\\nmodel message or string output into something more easily manipulated.The biggest benefit to raw prompting is its flexibility:Raw prompting does not require any special model features, only sufficient reasoning capability to understand\\nthe passed schema.You can prompt for any format you\\'d like, not just JSON. This can be useful if the model you\\nare using is more heavily trained on a certain type of data, such as XML or YAML.However, there are some drawbacks too:LLMs are non-deterministic, and prompting a LLM to consistently output data in the exactly correct format\\nfor smooth parsing can be surprisingly difficult and model-specific.Individual models have quirks depending on the data they were trained on, and optimizing prompts can be quite difficult.\\nSome may be better at interpreting JSON schema, others may be best with TypeScript definitions,\\nand still others may prefer XML.While we\\'ll next go over some ways that you can take advantage of features offered by\\nmodel providers to increase reliability, prompting techniques remain important for tuning your\\nresults no matter what method you choose.JSON mode\\u200bSome models, such as Mistral, OpenAI,\\nTogether AI and Ollama,\\nsupport a feature called JSON mode, usually enabled via config.When enabled, JSON mode will constrain the model\\'s output to always be some sort of valid JSON.\\nOften they require some custom prompting, but it\\'s usually much less burdensome and along the lines of,\\n\"you must always return JSON\", and the output is easier to parse.It\\'s also generally simpler and more commonly available than tool calling.Here\\'s an example:from langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain.output_parsers.json import SimpleJsonOutputParsermodel = ChatOpenAI(    model=\"gpt-4o\",    model_kwargs={ \"response_format\": { \"type\": \"json_object\" } },)prompt = ChatPromptTemplate.from_template(    \"Answer the user\\'s question to the best of your ability.\"    \\'You must always output a JSON object with an \"answer\" key and a \"followup_question\" key.\\'    \"{question}\")chain = prompt | model | SimpleJsonOutputParser()chain.invoke({ \"question\": \"What is the powerhouse of the cell?\" })API Reference:ChatPromptTemplate | ChatOpenAI | SimpleJsonOutputParser{\\'answer\\': \\'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.\\', \\'followup_question\\': \\'Would you like to know more about how mitochondria produce energy?\\'}For a full list of model providers that support JSON mode, see this table.Function/tool calling\\u200binfoWe use the term tool calling interchangeably with function calling. Although\\nfunction calling is sometimes meant to refer to invocations of a single function,\\nwe treat all models as though they can return multiple tool or function calls in\\neach messageTool calling allows a model to respond to a given prompt by generating output that\\nmatches a user-defined schema. While the name implies that the model is performing\\nsome action, this is actually not the case! The model is coming up with the\\narguments to a tool, and actually running the tool (or not) is up to the user -\\nfor example, if you want to extract output matching some schema\\nfrom unstructured text, you could give the model an \"extraction\" tool that takes\\nparameters matching the desired schema, then treat the generated output as your final\\nresult.For models that support it, tool calling can be very convenient. It removes the\\nguesswork around how best to prompt schemas in favor of a built-in model feature. It can also\\nmore naturally support agentic flows, since you can just pass multiple tool schemas instead\\nof fiddling with enums or unions.Many LLM providers, including Anthropic,\\nCohere, Google,\\nMistral, OpenAI, and others,\\nsupport variants of a tool calling feature. These features typically allow requests\\nto the LLM to include available tools and their schemas, and for responses to include\\ncalls to these tools. For instance, given a search engine tool, an LLM might handle a\\nquery by first issuing a call to the search engine. The system calling the LLM can\\nreceive the tool call, execute it, and return the output to the LLM to inform its\\nresponse. LangChain includes a suite of built-in tools\\nand supports several methods for defining your own custom tools.LangChain provides a standardized interface for tool calling that is consistent across different models.The standard interface consists of:ChatModel.bind_tools(): a method for specifying which tools are available for a model to call. This method accepts LangChain tools here.AIMessage.tool_calls: an attribute on the AIMessage returned from the model for accessing the tool calls requested by the model.The following how-to guides are good practical resources for using function/tool calling:How to return structured data from an LLMHow to use a model to call toolsFor a full list of model providers that support tool calling, see this table.Retrieval\\u200bLLMs are trained on a large but fixed dataset, limiting their ability to reason over private or recent information. Fine-tuning an LLM with specific facts is one way to mitigate this, but is often poorly suited for factual recall and can be costly.\\nRetrieval is the process of providing relevant information to an LLM to improve its response for a given input. Retrieval augmented generation (RAG) is the process of grounding the LLM generation (output) using the retrieved information.tipSee our RAG from Scratch code and video series.For a high-level guide on retrieval, see this tutorial on RAG.RAG is only as good as the retrieved documents’ relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems. We\\'ve focused on taxonomizing and summarizing many of these techniques (see below figure) and will share some high-level strategic guidance in the following sections.\\nYou can and should experiment with using different pieces together. You might also find this LangSmith guide useful for showing how to evaluate different iterations of your app.Query Translation\\u200bFirst, consider the user input(s) to your RAG system. Ideally, a RAG system can handle a wide range of inputs, from poorly worded questions to complex multi-part queries.\\nUsing an LLM to review and optionally modify the input is the central idea behind query translation. This serves as a general buffer, optimizing raw user inputs for your retrieval system.\\nFor example, this can be as simple as extracting keywords or as complex as generating multiple sub-questions for a complex query.NameWhen to useDescriptionMulti-queryWhen you need to cover multiple perspectives of a question.Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches.tipSee our RAG from Scratch videos for a few different specific approaches:Multi-queryDecompositionStep-backHyDERouting\\u200bSecond, consider the data sources available to your RAG system. You want to query across more than one database or across structured and unstructured data sources. Using an LLM to review the input and route it to the appropriate data source is a simple and effective approach for querying across sources.NameWhen to useDescriptionLogical routingWhen you can prompt an LLM with rules to decide where to route the input.Logical routing can use an LLM to reason about the query and choose which datastore is most appropriate.Semantic routingWhen semantic similarity is an effective way to determine where to route the input.Semantic routing embeds both query and, typically a set of prompts. It then chooses the appropriate prompt based upon similarity.tipSee our RAG from Scratch video on routing.  Query Construction\\u200bThird, consider whether any of your data sources require specific query formats. Many structured databases use SQL. Vector stores often have specific syntax for applying keyword filters to document metadata. Using an LLM to convert a natural language query into a query syntax is a popular and powerful approach.\\nIn particular, text-to-SQL, text-to-Cypher, and query analysis for metadata filters are useful ways to interact with structured, graph, and vector databases respectively. NameWhen to UseDescriptionText to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.Self QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).tipSee our blog post overview and RAG from Scratch video on query construction, the process of text-to-DSL where DSL is a domain specific language required to interact with a given database. This converts user questions into structured queries. Indexing\\u200bFouth, consider the design of your document index. A simple and powerful idea is to decouple the documents that you index for retrieval from the documents that you pass to the LLM for generation. Indexing frequently uses embedding models with vector stores, which compress the semantic information in documents to fixed-size vectors.Many RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question. Furthermore, LLMs are increasingly capable of processing millions of tokens. Two approaches can address this tension: (1) Multi Vector retriever using an LLM to translate documents into any form (e.g., often into a summary) that is well-suited for indexing, but returns full documents to the LLM for generation. (2) ParentDocument retriever embeds document chunks, but also returns full documents. The idea is to get the best of both worlds: use concise representations (summaries or chunks) for retrieval, but use the full documents for answer generation.NameIndex TypeUses an LLMWhen to UseDescriptionVector storeVector storeNoIf you are just getting started and looking for something quick and easy.This is the simplest method and the one that is easiest to get started with. It involves creating embeddings for each piece of text.ParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.Time-Weighted Vector storeVector storeNoIf you have timestamps associated with your documents, and you want to retrieve the most recent onesThis fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents)tipSee our RAG from Scratch video on indexing fundamentalsSee our RAG from Scratch video on multi vector retrieverFifth, consider ways to improve the quality of your similarity search itself. Embedding models compress text into fixed-length (vector) representations that capture the semantic content of the document. This compression is useful for search / retrieval, but puts a heavy burden on that single vector representation to capture the semantic nuance / detail of the document. In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding.ColBERT is an interesting approach to address this with a higher granularity embeddings: (1) produce a contextually influenced embedding for each token in the document and query, (2) score similarity between each query token and all document tokens, (3) take the max, (4) do this for all query tokens, and (5) take the sum of the max scores (in step 3) for all query tokens to get a query-document similarity score; this token-wise scoring can yield strong results. There are some additional tricks to improve the quality of your retrieval. Embeddings excel at capturing semantic information, but may struggle with keyword-based queries. Many vector stores offer built-in hybrid-search to combine keyword and semantic similarity, which marries the benefits of both approaches. Furthermore, many vector stores have maximal marginal relevance, which attempts to diversify the results of a search to avoid returning similar and redundant documents. NameWhen to useDescriptionColBERTWhen higher granularity embeddings are needed.ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score.Hybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.tipSee our RAG from Scratch video on ColBERT.Post-processing\\u200bSixth, consider ways to filter or rank retrieved documents. This is very useful if you are combining documents returned from multiple sources, since it can can down-rank less relevant documents and / or compress similar documents. NameIndex TypeUses an LLMWhen to UseDescriptionContextual CompressionAnySometimesIf you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.EnsembleAnyNoIf you have multiple retrieval methods and want to try combining them.This fetches documents from multiple retrievers and then combines them.Re-rankingAnyYesIf you want to rank retrieved documents based upon relevance, especially if you want to combine results from multiple retrieval methods .Given a query and a list of documents, Rerank indexes the documents from most to least semantically relevant to the query.tipSee our RAG from Scratch video on RAG-Fusion, on approach for post-processing across multiple queries:  Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, and combine the ranks of multiple search result lists to produce a single, unified ranking with Reciprocal Rank Fusion (RRF).Generation\\u200bFinally, consider ways to build self-correction into your RAG system. RAG systems can suffer from low quality retrieval (e.g., if a user question is out of the domain for the index) and / or hallucinations in generation. A naive retrieve-generate pipeline has no ability to detect or self-correct from these kinds of errors. The concept of \"flow engineering\" has been introduced in the context of code generation: iteratively build an answer to a code question with unit tests to check and self-correct errors. Several works have applied this RAG, such as Self-RAG and Corrective-RAG. In both cases, checks for document relevance, hallucinations, and / or answer quality are performed in the RAG answer generation flow.We\\'ve found that graphs are a great way to reliably express logical flows and have implemented ideas from several of these papers using LangGraph, as shown in the figure below (red - routing, blue - fallback, green - self-correction):Routing: Adaptive RAG (paper). Route questions to different retrieval approaches, as discussed above Fallback: Corrective RAG (paper). Fallback to web search if docs are not relevant to querySelf-correction: Self-RAG (paper). Fix answers w/ hallucinations or don’t address questionNameWhen to useDescriptionSelf-RAGWhen needing to fix answers with hallucinations or irrelevant content.Self-RAG performs checks for document relevance, hallucinations, and answer quality during the RAG answer generation flow, iteratively building an answer and self-correcting errors.Corrective-RAGWhen needing a fallback mechanism for low relevance docs.Corrective-RAG includes a fallback (e.g., to web search) if the retrieved documents are not relevant to the query, ensuring higher quality and more relevant retrieval.tipSee several videos and cookbooks showcasing RAG with LangGraph: LangGraph Corrective RAGLangGraph combining Adaptive, Self-RAG, and Corrective RAG Cookbooks for RAG using LangGraphSee our LangGraph RAG recipes with partners:Meta MistralText splitting\\u200bLangChain offers many different types of text splitters.\\nThese all live in the langchain-text-splitters package.Table columns:Name: Name of the text splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came from.Description: Description of the splitter, including recommendation on when to use it.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter, RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to keep related pieces of text next to each other. This is the recommended way to start splitting text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters✅Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitter,Markdown specific characters✅Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)Codemany languagesCode (Python, JS) specific charactersSplits text based on characters specific to coding languages. 15 different languages are available to choose from.Tokenmany classesTokensSplits text on tokens. There exist a few different ways to measure tokens.CharacterCharacterTextSplitterA user defined characterSplits text based on a user defined character. One of the simpler methods.Semantic Chunker (Experimental)SemanticChunkerSentencesFirst splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg KamradtIntegration: AI21 SemanticAI21SemanticTextSplitter✅Identifies distinct topics that form coherent pieces of text and splits along those.Evaluation\\u200bEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\\nIt involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\\nThis process is vital for building reliable applications.LangSmith helps with this process in a few ways:It makes it easier to create and curate datasets via its tracing and annotation featuresIt provides an evaluation framework that helps you define metrics and run your app against your datasetIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/CodeTo learn more, check out this LangSmith guide.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousHow to create and query vector storesNext🦜️🏓 LangServe', metadata={'source': 'https://python.langchain.com/v0.2/docs/concepts/'})]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "2df0ad9affdab903",
   "metadata": {},
   "source": [
    "在文档完成加载后我们需要对文档进行切分，这时候就需要text splitter\n",
    "#### Text Splitters\n",
    "前面已经解释过为啥要对文档进行切分了，但是很明显，对于文档切分应该会有很多策略，不同策略会有不同的效果。langchain的Text Splitters模块就提供了各种切分文档的策略。分割的策略对RAG来说非常重要，总的来说我们要降数据块变小的同时还要让数据块的语义相关，这两者其实本来就很矛盾。比如我有个句子\"William曾经做过两年python工程师，他后来又干了7年前端工程师\"。如果句子被切割为\"William曾经做过两年python工程师\"和\"他后来又干了7年前端工程师\"，那么你在提问\"William都干过些什么工作\"的时候，可能就只能找到第一条信息，得出他干过两年python的结论。而第二条信息因为他的指代，很可能在向量召回的时候并不能被召回。所以文档的分割需要根据应用场景进行权衡考虑。在langchain中提供了很多的Text Splitters，其中一些很直观，也很简单，比如ChatTextSplitter可以通过你提供的一个字符串(比如\\n\\t这种)，对文档进行分割。又或者NLTKTextSplitter和SpacyTextSplitter这种，可以以句子为单位进行分割。这些分割方法在合适的场景都有合适的用处，但是他们没有考虑语义，没有考虑前后两个块是否有语义上的关联，是否需要合并为一个chunk，因此langchain又实现了一个语义分割SemanticChunker，文档分割方法可以[参考文档](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)。我们这里以RecursiveCharacterTextSplitter为例来进行演示"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbaa26a696091c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:56:29.078780Z",
     "start_time": "2024-06-26T04:56:29.049844Z"
    }
   },
   "source": [
    "# Splitter.ts\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=10,\n",
    "    separators=['.', ' ', '\\n']\n",
    ")\n",
    "print(splitter.split_text(doc[0].page_content))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conceptual guideOn this pageConceptual guideThis section contains introductions to key parts of LangChain.Architecture\\u200bLangChain as a framework consists of a number of packages.langchain-core\\u200bThis package contains base abstractions of different components and ways to compose them together.\\nThe interfaces for core components like LLMs, vector stores, retrievers and more are defined here.\\nNo third party integrations are defined here.\\nThe dependencies are kept purposefully very lightweight', \".Partner packages\\u200bWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g. langchain-openai, langchain-anthropic, etc).\\nThis was done in order to improve support for these important integrations.langchain\\u200bThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\nThese are NOT third party integrations\", '.\\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.langchain-community\\u200bThis package contains third party integrations that are maintained by the LangChain community.\\nKey partner packages are separated out (see below).\\nThis contains all integrations for various components (LLMs, vector stores, retrievers).\\nAll dependencies in this package are optional to keep the package as lightweight as possible', '.langgraph\\u200blanggraph is an extension of langchain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.langserve\\u200bA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running', '.LangSmith\\u200bA developer platform that lets you debug, test, evaluate, and monitor LLM applications.LangChain Expression Language (LCEL)\\u200bLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production)', '. To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens', '.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server', '.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale', '. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server', '.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability', '.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Runnable interface\\u200bTo make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about below', '.This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way', '.\\nThe standard interface includes:stream: stream back chunks of the responseinvoke: call the chain on an inputbatch: call the chain on a list of inputsThese also have corresponding async methods that should be used with asyncio await syntax for concurrency:astream: stream back chunks of the response asyncainvoke: call the chain on an input asyncabatch: call the chain on a list of inputs asyncastream_log: stream back intermediate steps as they happen, in addition to the final responseastream_events: beta', 'beta stream events as they happen in the chain (introduced in langchain-core 0', '.1', '.14)The input type and output type varies by component:ComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the toolAll runnables expose input and output schemas to inspect the inputs and', 'and outputs:input_schema: an input Pydantic model auto-generated from the structure of the Runnableoutput_schema: an output Pydantic model auto-generated from the structure of the RunnableComponents\\u200bLangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs', '.\\nSome components LangChain implements, some components we rely on third-party integrations for, and others are a mix.Chat models\\u200bLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\\nThese are traditionally newer models (older models are generally LLMs, see below).\\nChat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages', '.Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input. This means you can easily use chat models in place of LLMs.When a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.LangChain does not host any Chat Models, rather we rely on third party integrations', '.We have some standardized parameters when constructing ChatModels:model: the name of the modeltemperature: the sampling temperaturetimeout: request timeoutmax_tokens: max tokens to generatestop: default stop sequencesmax_retries: max number of times to retry requestsapi_key: API key for the model providerbase_url: endpoint to send requests toSome important things to note:standard params only apply to model providers that expose parameters with the intended functionality', \". For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.standard params are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model\", '.infoTool Calling Some chat models have been fine-tuned for tool calling and provide a dedicated API for tool calling.\\nGenerally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\\nPlease see the tool calling section for more information.For specifics on how to use chat models, see the relevant how-to guides here.Multimodality\\u200bSome chat models are multimodal, accepting images, audio and even video as inputs', '. These are still less common, meaning model providers haven\\'t standardized on the \"best\" way to define the API. Multimodal outputs are even less common. As such, we\\'ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI\\'s content blocks format. So far this is restricted to image inputs', '. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.For specifics on how to use multimodal models, see the relevant how-to guides here.For a full list of LangChain model providers with multimodal models, check out this table.LLMs\\u200bcautionPure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as chat completion models,\\neven for non-chat use cases', '.You are probably looking for the section above instead.Language models that takes a string as input and returns a string.\\nThese are traditionally older models (newer models generally are Chat Models, see above).Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\\nThis gives them the same interface as Chat Models', '.\\nWhen messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.LangChain does not host any LLMs, rather we rely on third party integrations.For specifics on how to use LLMs, see the relevant how-to guides here.Messages\\u200bSome language models take a list of messages as input and return a message.\\nThere are a few different types of messages.\\nAll messages have a role, content, and response_metadata property', '.The role describes WHO is saying the message.\\nLangChain has different message classes for different roles.The content property describes the content of the message.\\nThis can be a few different things:A string (most models deal this type of content)A List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and that input location)HumanMessage\\u200bThis represents a message from the user.AIMessage\\u200bThis represents a message from the model', '. In addition to the content property, these messages also have:response_metadataThe response_metadata property contains additional metadata about the response. The data here is often specific to each model provider.\\nThis is where information like log-probs and token usage may be stored.tool_callsThese represent a decision from an language model to call a tool. They are included as part of an AIMessage output.\\nThey can be accessed from there with the .tool_calls property', '.This property returns a list of dictionaries. Each dictionary has the following keys:name: The name of the tool that should be called.args: The arguments to that tool.id: The id of that tool call.SystemMessage\\u200bThis represents a system message, which tells the model how to behave. Not every model provider supports this.FunctionMessage\\u200bThis represents the result of a function call', \". In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.ToolMessage\\u200bThis represents the result of a tool call. This is distinct from a FunctionMessage in order to match OpenAI's function and tool message types. In addition to role and content, this message has a tool_call_id parameter which conveys the id of the call to the tool that was called to produce this result\", \".Prompt templates\\u200bPrompt templates help to translate user input and parameters into instructions for a language model.\\nThis can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.Prompt Templates output a PromptValue\", '. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.There are a few different types of prompt templates:String PromptTemplates\\u200bThese prompt templates are used to format a single string, and generally are used for simpler inputs.\\nFor example, a common way to construct and use a PromptTemplate is as follows:from langchain_core', '.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplateChatPromptTemplates\\u200bThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:from langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate', '.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplateIn the above example, this ChatPromptTemplate will construct two messages when called.\\nThe first is a system message, that has no variables to format.\\nThe second is a HumanMessage, and will be formatted by the topic variable the user passes in', '.MessagesPlaceholder\\u200bThis prompt template is responsible for adding a list of messages in a particular place.\\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\\nThis is how you use MessagesPlaceholder.from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate', '.from_messages([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessageThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in)', '.\\nThis is useful for letting a list of messages be slotted into a particular spot.An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:prompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])For specifics on how to use prompt templates, see the relevant how-to guides here', '.Example selectors\\u200bOne common prompting technique for achieving better performance is to include examples as part of the prompt.\\nThis gives the language model concrete examples of how it should behave.\\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\\nExample Selectors are classes responsible for selecting and then formatting examples into prompts', '.For specifics on how to use example selectors, see the relevant how-to guides here.Output parsers\\u200bnoteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\nMore and more models are supporting function (or tool) calling, which handles this automatically.\\nIt is recommended to use function/tool calling rather than output parsing.\\nSee documentation for that here', '.Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:Name: The name of the output parserSupports Streaming: Whether the output parser supports streaming', '.Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.Input Type: Expected input type', '. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.Output Type: The output type of the object returned by the parser.Description: Our commentary on this output parser and when to use it.NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionJSON✅✅str | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model', \". Probably the most reliable output parser for getting structured data that does NOT use function calling.XML✅✅str | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).CSV✅✅str | MessageList[str]Returns a list of comma separated values.OutputFixing✅str | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output\", '.RetryWithError✅str | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML✅str | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it', '.PandasDataFrame✅str | MessagedictUseful for doing operations with pandas DataFrames.Enum✅str | MessageEnumParses response into one of the provided enum values.Datetime✅str | Messagedatetime.datetimeParses response into a datetime string.Structured✅str | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs', '.For specifics on how to use output parsers, see the relevant how-to guides here.Chat history\\u200bMost LLM applications have a conversational interface.\\nAn essential component of a conversation is being able to refer to information introduced earlier in the conversation.\\nAt bare minimum, a conversational system should be able to access some window of past messages directly.The concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain', '.\\nThis ChatHistory will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database.\\nFuture interactions will then load those messages and pass them into the chain as part of the input.Documents\\u200bA Document object in LangChain contains information about some data. It has two attributes:page_content: str: The content of this document. Currently is only a string.metadata: dict: Arbitrary metadata associated with this document', '. Can track the document id, file name, etc.Document loaders\\u200bThese classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\\nAn example use case is as follows:from langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(    ..', \"...  # <-- Integration specific parameters here)data = loader.load()API Reference:CSVLoaderFor specifics on how to use document loaders, see the relevant how-to guides here.Text splitters\\u200bOnce you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window\", '. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that', '.At a high level, text splitters work as following:Split the text up into small, semantically meaningful chunks (often sentences).Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks)', '.That means there are two different axes along which you can customize your text splitter:How the text is splitHow the chunk size is measuredFor specifics on how to use text splitters, see the relevant how-to guides here.Embedding models\\u200bEmbedding models create a vector representation of a piece of text. You can think of a vector as an array of numbers that captures the semantic meaning of the text', '.\\nBy representing the text in this way, you can perform mathematical operations that allow you to do things like search for other pieces of text that are most similar in meaning.\\nThese natural language search capabilities underpin many types of context retrieval,\\nwhere we provide an LLM with the relevant data it needs to effectively respond to a query.The Embeddings class is a class designed for interfacing with text embedding models', '. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text', '. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).For specifics on how to use embedding models, see the relevant how-to guides here', \".Vector stores\\u200bOne of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors,\\nand then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query.\\nA vector store takes care of storing embedded data and performing vector search for you\", '.Most vector stores can also store metadata about embedded vectors and support filtering on that metadata before\\nsimilarity search, allowing you more control over returned documents.Vector stores can be converted to the retriever interface by doing:vectorstore = MyVectorStore()retriever = vectorstore.as_retriever()For specifics on how to use vector stores, see the relevant how-to guides here.Retrievers\\u200bA retriever is an interface that returns documents given an unstructured query', \".\\nIt is more general than a vector store.\\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\\nRetrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.Retrievers accept a string query as input and return a list of Document's as output.For specifics on how to use retrievers, see the relevant how-to guides here\", '.Tools\\u200bTools are interfaces that an agent, a chain, or a chat model / LLM can use to interact with the world.A tool consists of the following components:The name of the toolA description of what the tool doesJSON schema of what the inputs to the tool areThe function to callWhether the result of a tool should be returned directly to the user (only relevant for agents)The name, description and JSON schema are provided as context\\nto the LLM, allowing the LLM to determine how to use the tool\\nappropriately', '.Given a list of available tools and a prompt, an LLM can request\\nthat one or more tools be invoked with appropriate arguments.Generally, when designing tools to be used by a chat model or LLM, it is important to keep in mind the following:Chat models that have been fine-tuned for tool calling will be better at tool calling than non-fine-tuned models.Non fine-tuned models may not be able to use tools at all, especially if the tools are complex or require multiple tool calls', '.Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas.Simpler tools are generally easier for models to use than more complex tools.For specifics on how to use tools, see the relevant how-to guides here.Toolkits\\u200bToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.All Toolkits expose a get_tools method which returns a list of tools', \".\\nYou can therefore do:# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()Agents\\u200bBy themselves, language models can't take actions - they just output text.\\nA big use case for LangChain is creating agents.\\nAgents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be\", '.\\nThe results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.\\nPlease check out that documentation for a more in depth overview of agent concepts.There is a legacy agent concept in LangChain that we are moving towards deprecating: AgentExecutor.\\nAgentExecutor was essentially a runtime for agents', '.\\nIt was a great place to get started, however, it was not flexible enough as you started to have more customized agents.\\nIn order to solve that we built LangGraph to be this flexible, highly-controllable runtime.If you are still using AgentExecutor, do not fear: we still have a guide on how to use AgentExecutor.\\nIt is recommended, however, that you start to transition to LangGraph.\\nIn order to assist in this we have put together a transition guide on how to do so', '.Callbacks\\u200bLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail', '.Callback Events\\u200bEventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool', 'endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retryCallback handlers\\u200bCallback handlers can either be sync or async:Sync callback handlers implement the', 'the BaseCallbackHandler interface', '.Async callback handlers implement the AsyncCallbackHandler interface.During run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.Passing callbacks\\u200bThe callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc', '.) in two different places:The callbacks are available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:Request time callbacks: Passed at the time of the request in addition to the input data.\\nAvailable on all standard Runnable objects. These callbacks are INHERITED by all children\\nof the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).Constructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler])', \". These callbacks\\nare passed as arguments to the constructor of the object. The callbacks are scoped\\nonly to the object they are defined on, and are not inherited by any children of the object.dangerConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\\nof the object.If you're creating a custom chain or runnable, you need to remember to propagate request time\\ncallbacks to any child objects.Async in Python<=3\", '.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\\nand is running async in python<=3.10, will have to propagate callbacks to child\\nobjects manually. This is because LangChain cannot automatically propagate\\ncallbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\\nrunnables or tools.For specifics on how to use callbacks, see the relevant how-to guides here', \".Techniques\\u200bStreaming\\u200bIndividual LLM calls often run for much longer than traditional resource requests.\\nThis compounds when you build more complex chains or agents that require multiple reasoning steps.Fortunately, LLMs generate output iteratively, which means it's possible to show sensible intermediate results\\nbefore the final response is ready\", \". Consuming output as soon as it becomes available has therefore become a vital part of the UX\\naround building apps with LLMs to help alleviate latency issues, and LangChain aims to have first-class support for streaming.Below, we'll discuss some concepts and considerations around streaming in LangChain..stream() and .astream()\\u200bMost modules in LangChain include the .stream() method (and the equivalent .astream() method for async environments) as an ergonomic streaming interface.\", '.\\n.stream() returns an iterator, which you can consume with a simple for loop. Here\\'s an example with a chat model:from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")for chunk in model.stream(\"what color is the sky?\"):    print(chunk', '.content, end=\"|\", flush=True)API Reference:ChatAnthropicFor models (or other components) that don\\'t support streaming natively, this iterator would just yield a single chunk, but\\nyou could still use the same general pattern when calling them. Using .stream() will also automatically call the model in streaming mode\\nwithout the need to provide additional config.The type of each outputted chunk depends on the type of component - for example, chat models yield AIMessageChunks', '.\\nBecause this method is part of LangChain Expression Language,\\nyou can handle formatting differences from different outputs using an output parser to transform\\neach yielded chunk.You can check out this guide for more detail on how to use .stream()..astream_events()\\u200bWhile the .stream() method is intuitive, it can only return the final generated value of your chain', '. This is fine for single LLM calls,\\nbut as you build more complex chains of several LLM calls together, you may want to use the intermediate values of\\nthe chain alongside the final output - for example, returning sources alongside the final generation when building a chat\\nover documents app.There are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate\\nvalues to the end with something like chained .assign() calls, but LangChain also includes an', \".astream_events() method that combines the flexibility of callbacks with the ergonomics of .stream(). When called, it returns an iterator\\nwhich yields various types of events that you can filter and process according\\nto the needs of your project.Here's one small example that prints just events containing streamed chat model output:from langchain_core.output_parsers import StrOutputParserfrom langchain_core\", '.prompts import ChatPromptTemplatefrom langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for event in chain', '.astream_events({\"topic\": \"parrot\"}, version=\"v2\"):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event, end=\"|\", flush=True)API Reference:StrOutputParser | ChatPromptTemplate | ChatAnthropicYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!See this guide for more detailed information on how to use .astream_events(),\\nincluding a table listing available events', '.Callbacks\\u200bThe lowest level way to stream outputs from LLMs in LangChain is via the callbacks system. You can pass a\\ncallback handler that handles the on_llm_new_token event into LangChain components. When that component is invoked, any\\nLLM or chat model contained in the component calls\\nthe callback with the generated token. Within the callback, you could pipe the tokens into some other destination, e.g. a HTTP response.\\nYou can also handle the on_llm_end event to perform any necessary cleanup', \".You can see this how-to section for more specifics on using callbacks.Callbacks were the first technique for streaming introduced in LangChain. While powerful and generalizable,\\nthey can be unwieldy for developers. For example:You need to explicitly initialize and manage some aggregator or other stream to collect results.The execution order isn't explicitly guaranteed, and you could theoretically have a callback run after the .invoke() method finishes\", '.Providers would often make you pass an additional parameter to stream outputs instead of returning them all at once.You would often ignore the result of the actual model call in favor of callback results.Tokens\\u200bThe unit that most model providers use to measure input and output is via a unit called a token.\\nTokens are the basic units that language models read and generate when processing or producing text', '.\\nThe exact definition of a token can vary depending on the specific way the model was trained -\\nfor instance, in English, a token could be a single word like \"apple\", or a part of a word like \"app\".When you send a model a prompt, the words and characters in the prompt are encoded into tokens using a tokenizer.\\nThe model then streams back generated output tokens, which the tokenizer decodes into human-readable text', '.\\nThe below example shows how OpenAI models tokenize LangChain is cool!:You can see that it gets split into 5 different tokens, and that the boundaries between tokens are not exactly the same as word boundaries.The reason language models use tokens rather than something more immediately intuitive like \"characters\"\\nhas to do with how they process and understand text. At a high-level, language models iteratively predict their next generated output based on\\nthe initial input and their previous generations', '. Training the model using tokens language models to handle linguistic\\nunits (like words or subwords) that carry meaning, rather than individual characters, which makes it easier for the model\\nto learn and understand the structure of the language, including grammar and context.\\nFurthermore, using tokens can also improve efficiency, since the model processes fewer units of text compared to character-level processing.Structured output\\u200bLLMs are capable of generating arbitrary text', \". This enables the model to respond appropriately to a wide\\nrange of inputs, but for some use-cases, it can be useful to constrain the LLM's output\\nto a specific format or structure. This is referred to as structured output.For example, if the output is to be stored in a relational database,\\nit is much easier if the model generates output that adheres to a defined schema or format.\\nExtracting specific information from unstructured text is another\\ncase where this is particularly useful\", \". Most commonly, the output format will be JSON,\\nthough other formats such as YAML can be useful too. Below, we'll discuss\\na few ways to get structured output from models in LangChain..with_structured_output()\\u200bFor convenience, some LangChain chat models support a .with_structured_output() method.\\nThis method only requires a schema as input, and returns a dict or Pydantic object\", '.\\nGenerally, this method is only present on models that support one of the more advanced methods described below,\\nand will use one of them under the hood. It takes care of importing a suitable output parser and\\nformatting the schema in the right format for the model.For more information, check out this how-to guide.Raw prompting\\u200bThe most intuitive way to get a model to structure output is to ask nicely', \".\\nIn addition to your query, you can give instructions describing what kind of output you'd like, then\\nparse the output using an output parser to convert the raw\\nmodel message or string output into something more easily manipulated.The biggest benefit to raw prompting is its flexibility:Raw prompting does not require any special model features, only sufficient reasoning capability to understand\\nthe passed schema.You can prompt for any format you'd like, not just JSON\", '. This can be useful if the model you\\nare using is more heavily trained on a certain type of data, such as XML or YAML.However, there are some drawbacks too:LLMs are non-deterministic, and prompting a LLM to consistently output data in the exactly correct format\\nfor smooth parsing can be surprisingly difficult and model-specific.Individual models have quirks depending on the data they were trained on, and optimizing prompts can be quite difficult', \".\\nSome may be better at interpreting JSON schema, others may be best with TypeScript definitions,\\nand still others may prefer XML.While we'll next go over some ways that you can take advantage of features offered by\\nmodel providers to increase reliability, prompting techniques remain important for tuning your\\nresults no matter what method you choose.JSON mode\\u200bSome models, such as Mistral, OpenAI,\\nTogether AI and Ollama,\\nsupport a feature called JSON mode, usually enabled via config\", '.When enabled, JSON mode will constrain the model\\'s output to always be some sort of valid JSON.\\nOften they require some custom prompting, but it\\'s usually much less burdensome and along the lines of,\\n\"you must always return JSON\", and the output is easier to parse.It\\'s also generally simpler and more commonly available than tool calling.Here\\'s an example:from langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain.output_parsers', '.json import SimpleJsonOutputParsermodel = ChatOpenAI(    model=\"gpt-4o\",    model_kwargs={ \"response_format\": { \"type\": \"json_object\" } },)prompt = ChatPromptTemplate.from_template(    \"Answer the user\\'s question to the best of your ability.\"    \\'You must always output a JSON object with an \"answer\" key and a \"followup_question\" key.\\'    \"{question}\")chain = prompt | model | SimpleJsonOutputParser()chain', '.invoke({ \"question\": \"What is the powerhouse of the cell?\" })API Reference:ChatPromptTemplate | ChatOpenAI | SimpleJsonOutputParser{\\'answer\\': \\'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.\\', \\'followup_question\\': \\'Would you like to know more about how mitochondria produce energy?\\'}For a full list of model providers that support JSON mode, see this table', '.Function/tool calling\\u200binfoWe use the term tool calling interchangeably with function calling. Although\\nfunction calling is sometimes meant to refer to invocations of a single function,\\nwe treat all models as though they can return multiple tool or function calls in\\neach messageTool calling allows a model to respond to a given prompt by generating output that\\nmatches a user-defined schema', '. While the name implies that the model is performing\\nsome action, this is actually not the case! The model is coming up with the\\narguments to a tool, and actually running the tool (or not) is up to the user -\\nfor example, if you want to extract output matching some schema\\nfrom unstructured text, you could give the model an \"extraction\" tool that takes\\nparameters matching the desired schema, then treat the generated output as your final\\nresult.For models that support it, tool calling can be very convenient', '. It removes the\\nguesswork around how best to prompt schemas in favor of a built-in model feature. It can also\\nmore naturally support agentic flows, since you can just pass multiple tool schemas instead\\nof fiddling with enums or unions.Many LLM providers, including Anthropic,\\nCohere, Google,\\nMistral, OpenAI, and others,\\nsupport variants of a tool calling feature. These features typically allow requests\\nto the LLM to include available tools and their schemas, and for responses to include\\ncalls to these tools', '. For instance, given a search engine tool, an LLM might handle a\\nquery by first issuing a call to the search engine. The system calling the LLM can\\nreceive the tool call, execute it, and return the output to the LLM to inform its\\nresponse. LangChain includes a suite of built-in tools\\nand supports several methods for defining your own custom tools.LangChain provides a standardized interface for tool calling that is consistent across different models.The standard interface consists of:ChatModel', '.bind_tools(): a method for specifying which tools are available for a model to call. This method accepts LangChain tools here.AIMessage.tool_calls: an attribute on the AIMessage returned from the model for accessing the tool calls requested by the model.The following how-to guides are good practical resources for using function/tool calling:How to return structured data from an LLMHow to use a model to call toolsFor a full list of model providers that support tool calling, see this table', '.Retrieval\\u200bLLMs are trained on a large but fixed dataset, limiting their ability to reason over private or recent information. Fine-tuning an LLM with specific facts is one way to mitigate this, but is often poorly suited for factual recall and can be costly.\\nRetrieval is the process of providing relevant information to an LLM to improve its response for a given input. Retrieval augmented generation (RAG) is the process of grounding the LLM generation (output) using the retrieved information', \".tipSee our RAG from Scratch code and video series.For a high-level guide on retrieval, see this tutorial on RAG.RAG is only as good as the retrieved documents’ relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems. We've focused on taxonomizing and summarizing many of these techniques (see below figure) and will share some high-level strategic guidance in the following sections.\\nYou can and should experiment with using different pieces together\", '. You might also find this LangSmith guide useful for showing how to evaluate different iterations of your app.Query Translation\\u200bFirst, consider the user input(s) to your RAG system. Ideally, a RAG system can handle a wide range of inputs, from poorly worded questions to complex multi-part queries.\\nUsing an LLM to review and optionally modify the input is the central idea behind query translation. This serves as a general buffer, optimizing raw user inputs for your retrieval system', '.\\nFor example, this can be as simple as extracting keywords or as complex as generating multiple sub-questions for a complex query.NameWhen to useDescriptionMulti-queryWhen you need to cover multiple perspectives of a question.Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems', '.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question', '.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches.tipSee our RAG from Scratch videos for a few different specific approaches:Multi-queryDecompositionStep-backHyDERouting\\u200bSecond, consider the data sources available to your RAG system', '. You want to query across more than one database or across structured and unstructured data sources. Using an LLM to review the input and route it to the appropriate data source is a simple and effective approach for querying across sources.NameWhen to useDescriptionLogical routingWhen you can prompt an LLM with rules to decide where to route the input.Logical routing can use an LLM to reason about the query and choose which datastore is most appropriate', '.Semantic routingWhen semantic similarity is an effective way to determine where to route the input.Semantic routing embeds both query and, typically a set of prompts. It then chooses the appropriate prompt based upon similarity.tipSee our RAG from Scratch video on routing.  Query Construction\\u200bThird, consider whether any of your data sources require specific query formats. Many structured databases use SQL. Vector stores often have specific syntax for applying keyword filters to document metadata', '. Using an LLM to convert a natural language query into a query syntax is a popular and powerful approach.\\nIn particular, text-to-SQL, text-to-Cypher, and query analysis for metadata filters are useful ways to interact with structured, graph, and vector databases respectively. NameWhen to UseDescriptionText to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query', '.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.Self QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it', '. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).tipSee our blog post overview and RAG from Scratch video on query construction, the process of text-to-DSL where DSL is a domain specific language required to interact with a given database. This converts user questions into structured queries. Indexing\\u200bFouth, consider the design of your document index', '. A simple and powerful idea is to decouple the documents that you index for retrieval from the documents that you pass to the LLM for generation. Indexing frequently uses embedding models with vector stores, which compress the semantic information in documents to fixed-size vectors.Many RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM', '. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question. Furthermore, LLMs are increasingly capable of processing millions of tokens. Two approaches can address this tension: (1) Multi Vector retriever using an LLM to translate documents into any form (e.g., often into a summary) that is well-suited for indexing, but returns full documents to the LLM for generation', '. (2) ParentDocument retriever embeds document chunks, but also returns full documents. The idea is to get the best of both worlds: use concise representations (summaries or chunks) for retrieval, but use the full documents for answer generation.NameIndex TypeUses an LLMWhen to UseDescriptionVector storeVector storeNoIf you are just getting started and looking for something quick and easy.This is the simplest method and the one that is easiest to get started with', '. It involves creating embeddings for each piece of text.ParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks)', '.Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions', '.Time-Weighted Vector storeVector storeNoIf you have timestamps associated with your documents, and you want to retrieve the most recent onesThis fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents)tipSee our RAG from Scratch video on indexing fundamentalsSee our RAG from Scratch video on multi vector retrieverFifth, consider ways to improve the quality of your similarity search itself', '. Embedding models compress text into fixed-length (vector) representations that capture the semantic content of the document. This compression is useful for search / retrieval, but puts a heavy burden on that single vector representation to capture the semantic nuance / detail of the document. In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding', '.ColBERT is an interesting approach to address this with a higher granularity embeddings: (1) produce a contextually influenced embedding for each token in the document and query, (2) score similarity between each query token and all document tokens, (3) take the max, (4) do this for all query tokens, and (5) take the sum of the max scores (in step 3) for all query tokens to get a query-document similarity score; this token-wise scoring can yield strong results', '. There are some additional tricks to improve the quality of your retrieval. Embeddings excel at capturing semantic information, but may struggle with keyword-based queries. Many vector stores offer built-in hybrid-search to combine keyword and semantic similarity, which marries the benefits of both approaches. Furthermore, many vector stores have maximal marginal relevance, which attempts to diversify the results of a search to avoid returning similar and redundant documents', '. NameWhen to useDescriptionColBERTWhen higher granularity embeddings are needed.ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score.Hybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Maximal Marginal Relevance (MMR)When needing to diversify search results', '.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.tipSee our RAG from Scratch video on ColBERT.Post-processing\\u200bSixth, consider ways to filter or rank retrieved documents. This is very useful if you are combining documents returned from multiple sources, since it can can down-rank less relevant documents and / or compress similar documents', '. NameIndex TypeUses an LLMWhen to UseDescriptionContextual CompressionAnySometimesIf you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.EnsembleAnyNoIf you have multiple retrieval methods and want to try combining them', '.This fetches documents from multiple retrievers and then combines them.Re-rankingAnyYesIf you want to rank retrieved documents based upon relevance, especially if you want to combine results from multiple retrieval methods .Given a query and a list of documents, Rerank indexes the documents from most to least semantically relevant to the query', '.tipSee our RAG from Scratch video on RAG-Fusion, on approach for post-processing across multiple queries:  Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, and combine the ranks of multiple search result lists to produce a single, unified ranking with Reciprocal Rank Fusion (RRF).Generation\\u200bFinally, consider ways to build self-correction into your RAG system. RAG systems can suffer from low quality retrieval (e.g', '.g., if a user question is out of the domain for the index) and / or hallucinations in generation. A naive retrieve-generate pipeline has no ability to detect or self-correct from these kinds of errors. The concept of \"flow engineering\" has been introduced in the context of code generation: iteratively build an answer to a code question with unit tests to check and self-correct errors. Several works have applied this RAG, such as Self-RAG and Corrective-RAG', \". In both cases, checks for document relevance, hallucinations, and / or answer quality are performed in the RAG answer generation flow.We've found that graphs are a great way to reliably express logical flows and have implemented ideas from several of these papers using LangGraph, as shown in the figure below (red - routing, blue - fallback, green - self-correction):Routing: Adaptive RAG (paper). Route questions to different retrieval approaches, as discussed above Fallback: Corrective RAG (paper)\", '. Fallback to web search if docs are not relevant to querySelf-correction: Self-RAG (paper). Fix answers w/ hallucinations or don’t address questionNameWhen to useDescriptionSelf-RAGWhen needing to fix answers with hallucinations or irrelevant content.Self-RAG performs checks for document relevance, hallucinations, and answer quality during the RAG answer generation flow, iteratively building an answer and self-correcting errors.Corrective-RAGWhen needing a fallback mechanism for low relevance docs', '.Corrective-RAG includes a fallback (e.g., to web search) if the retrieved documents are not relevant to the query, ensuring higher quality and more relevant retrieval.tipSee several videos and cookbooks showcasing RAG with LangGraph: LangGraph Corrective RAGLangGraph combining Adaptive, Self-RAG, and Corrective RAG Cookbooks for RAG using LangGraphSee our LangGraph RAG recipes with partners:Meta MistralText splitting\\u200bLangChain offers many different types of text splitters', '.\\nThese all live in the langchain-text-splitters package.Table columns:Name: Name of the text splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came from.Description: Description of the splitter, including recommendation on when to use it', '.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter, RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to keep related pieces of text next to each other. This is the recommended way to start splitting text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters✅Splits text based on HTML-specific characters', '. Notably, this adds in relevant information about where that chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitter,Markdown specific characters✅Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)Codemany languagesCode (Python, JS) specific charactersSplits text based on characters specific to coding languages. 15 different languages are available to choose from', '.Tokenmany classesTokensSplits text on tokens. There exist a few different ways to measure tokens.CharacterCharacterTextSplitterA user defined characterSplits text based on a user defined character. One of the simpler methods.Semantic Chunker (Experimental)SemanticChunkerSentencesFirst splits on sentences. Then combines ones next to each other if they are semantically similar enough', \". Taken from Greg KamradtIntegration: AI21 SemanticAI21SemanticTextSplitter✅Identifies distinct topics that form coherent pieces of text and splits along those.Evaluation\\u200bEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\\nIt involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\\nThis process is vital for building reliable applications\", '.LangSmith helps with this process in a few ways:It makes it easier to create and curate datasets via its tracing and annotation featuresIt provides an evaluation framework that helps you define metrics and run your app against your datasetIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/CodeTo learn more, check out this LangSmith guide.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub', '.PreviousHow to create and query vector storesNext🦜️🏓 LangServe']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "e2683a1b62fdb67a",
   "metadata": {},
   "source": [
    "其中chunk_size表示分割后块的大小的最大值，分割后块大小不会超过这个值。chunk_overlap指的是两个相邻文档块的最大重合字符数。比如之前的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787214b268848926",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "817228642bce6a1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:56:29.109586Z",
     "start_time": "2024-06-26T04:56:29.091786Z"
    }
   },
   "source": [
    "# Splitter.ts\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=40,\n",
    "    chunk_overlap=14,\n",
    "    separators=['。']\n",
    ")\n",
    "splitter.split_text('William曾经做过两年python工程师。他后来又干了七年前端工程师。现在他学起了java。')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['William曾经做过两年python工程师。他后来又干了七年前端工程师', '。他后来又干了七年前端工程师。现在他学起了java。']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "3b2ac18364a618e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e6a2883c1d507d",
   "metadata": {},
   "source": [
    "#### Embedding和Vector Store\n",
    "Embedding之前在分享Wolfram的《这就是ChatGPT》的时候就详细介绍过了，因此这里只是简单说下，Embedding模型就是一种向量化的算法模型，它向量化的对象可以是文档，可以是音频，视频，它可以提取对象的特定特征，虽然并不能解释它的特征具体是啥，但是它具有相似的对象在向量上也有更近相似度的特点。embedding模型的选型可以参考Huggingface的[LeaderBoard](https://huggingface.co/spaces/mteb/leaderboard)，我们这里演示为了方便采用的是通义千问的embedding模型text-embedding-v2。"
   ]
  },
  {
   "cell_type": "code",
   "id": "b77bb381c28ca6ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:56:29.831482Z",
     "start_time": "2024-06-26T04:56:29.112585Z"
    }
   },
   "source": [
    "# Embedding.ts\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model='text-embedding-v2',\n",
    "    dashscope_api_key='sk-90cba3edc3e1412b84547915475dca30'\n",
    ")\n",
    "v = embeddings.embed_query(\"William曾经做过两年python工程师。他后来又干了七年前端工程师。现在他学起了java。\")\n",
    "len(v)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "ad7b9c672d046e90",
   "metadata": {},
   "source": [
    "关于向量数据库，目前也有很多选择，这次我们使用的是Pinecone，因为它是我接触的第一个向量数据库，是以服务方式提供，有免费资源，也有UI界面可以清楚看到结果，比较适合初学。在上手之后可以去学习一些开源的离线部署的向量库，比如milvus，Lancedb，chroma这种。langchain也支持了市面上几乎所有的向量库的连接，具体库的连接就[参考文档](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d464b7924c70e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T05:00:09.753475Z",
     "start_time": "2024-06-26T04:56:29.835030Z"
    }
   },
   "source": [
    "# RAGOffline.ts\n",
    "import bs4\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import getpass\n",
    "\n",
    "if os.environ.get('PINECONE_API_KEY') is None:\n",
    "    os.environ['PINECONE_API_KEY'] = getpass.getpass('PINECONE_API_KEY: ')\n",
    "\n",
    "index_name = 'share-langchain-rag'\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path='https://www.gov.cn/flfg/2013-02/08/content_2332395.htm',\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "        class_=\"p1\"\n",
    "    ))\n",
    ")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model='text-embedding-v2',\n",
    "    dashscope_api_key='sk-90cba3edc3e1412b84547915475dca30'\n",
    ")\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "a96f5a54-30e3-412c-82ef-1ef4780062b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T05:00:12.152272Z",
     "start_time": "2024-06-26T05:00:09.755918Z"
    }
   },
   "source": [
    "docsearch.similarity_search(\"根据我国著作权保护法，软件保护期是多久\", k=10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='法人或者其他组织的软件著作权，保护期为50年，截止于软件首次发表后第50年的12月31日，但软件自开发完成之日起50年内未发表的，本条例不再保护。\\n\\xa0\\xa0\\xa0 第十五条 软件著作权属于自然人的，该自然人死亡后，在软件著作权的保护期内，软件著作权的继承人可以依照《中华人民共和国继承法》的有关规定，继承本条例第八条规定的除署名权以外的其他权利。\\n\\xa0\\xa0\\xa0\\xa0软件著作权属于法人或者其他组织的，法人或者其他组织变更、终止后，其著作权在本条例规定的保护期内由承受其权利义务的法人或者其他组织享有；没有承受其权利义务的法人或者其他组织的，由国家享有。\\n\\xa0\\xa0\\xa0 第十六条 软件的合法复制品所有人享有下列权利：\\n\\xa0\\xa0\\xa0 （一）根据使用的需要把该软件装入计算机等具有信息处理能力的装置内；\\n\\xa0\\xa0\\xa0 （二）为了防止复制品损坏而制作备份复制品。这些备份复制品不得通过任何方式提供给他人使用，并在所有人丧失该合法复制品的所有权时，负责将备份复制品销毁；\\n\\xa0\\xa0\\xa0 （三）为了把该软件用于实际的计算机应用环境或者改进其功能、性能而进行必要的修改；但是，除合同另有约定外，未经该软件著作权人许可，不得向任何第三方提供修改后的软件。\\n\\xa0\\xa0\\xa0 第十七条 为了学习和研究软件内含的设计思想和原理，通过安装、显示、传输或者存储软件等方式使用软件的，可以不经软件著作权人许可，不向其支付报酬。\\n第三章 软件著作权的许可使用和转让\\xa0\\n\\xa0\\xa0\\xa0\\xa0第十八条 许可他人行使软件著作权的，应当订立许可使用合同。\\n\\xa0\\xa0\\xa0 许可使用合同中软件著作权人未明确许可的权利，被许可人不得行使。\\n\\xa0\\xa0\\xa0 第十九条 许可他人专有行使软件著作权的，当事人应当订立书面合同。\\n\\xa0\\xa0\\xa0 没有订立书面合同或者合同中未明确约定为专有许可的，被许可行使的权利应当视为非专有权利。\\n\\xa0\\xa0\\xa0 第二十条 转让软件著作权的，当事人应当订立书面合同。\\n\\xa0\\xa0\\xa0 第二十一条 订立许可他人专有行使软件著作权的许可合同，或者订立转让软件著作权合同，可以向国务院著作权行政管理部门认定的软件登记机构登记。\\n\\xa0\\xa0\\xa0 第二十二条 中国公民、法人或者其他组织向外国人许可或者转让软件著作权的，应当遵守《中华人民共和国技术进出口管理条例》的有关规定。\\n第四章 法律责任', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='（九）应当由软件著作权人享有的其他权利。\\n\\xa0\\xa0\\xa0\\xa0软件著作权人可以许可他人行使其软件著作权，并有权获得报酬。\\n\\xa0\\xa0\\xa0 软件著作权人可以全部或者部分转让其软件著作权，并有权获得报酬。\\n\\xa0\\xa0\\xa0 第九条 软件著作权属于软件开发者，本条例另有规定的除外。\\n\\xa0\\xa0\\xa0 如无相反证明，在软件上署名的自然人、法人或者其他组织为开发者。\\n\\xa0\\xa0\\xa0 第十条 由两个以上的自然人、法人或者其他组织合作开发的软件，其著作权的归属由合作开发者签订书面合同约定。无书面合同或者合同未作明确约定，合作开发的软件可以分割使用的，开发者对各自开发的部分可以单独享有著作权；但是，行使著作权时，不得扩展到合作开发的软件整体的著作权。合作开发的软件不能分割使用的，其著作权由各合作开发者共同享有，通过协商一致行使；不能协商一致，又无正当理由的，任何一方不得阻止他方行使除转让权以外的其他权利，但是所得收益应当合理分配给所有合作开发者。\\n\\xa0\\xa0\\xa0 第十一条 接受他人委托开发的软件，其著作权的归属由委托人与受托人签订书面合同约定；无书面合同或者合同未作明确约定的，其著作权由受托人享有。\\n\\xa0\\xa0\\xa0 第十二条 由国家机关下达任务开发的软件，著作权的归属与行使由项目任务书或者合同规定；项目任务书或者合同中未作明确规定的，软件著作权由接受任务的法人或者其他组织享有。\\n\\xa0\\xa0\\xa0 第十三条 自然人在法人或者其他组织中任职期间所开发的软件有下列情形之一的，该软件著作权由该法人或者其他组织享有，该法人或者其他组织可以对开发软件的自然人进行奖励：\\n\\xa0\\xa0\\xa0 （一）针对本职工作中明确指定的开发目标所开发的软件；\\n\\xa0\\xa0\\xa0 （二）开发的软件是从事本职工作活动所预见的结果或者自然的结果；\\n\\xa0\\xa0\\xa0 （三）主要使用了法人或者其他组织的资金、专用设备、未公开的专门信息等物质技术条件所开发并由法人或者其他组织承担责任的软件。\\n\\xa0\\xa0\\xa0 第十四条 软件著作权自软件开发完成之日起产生。\\n\\xa0\\xa0\\xa0 自然人的软件著作权，保护期为自然人终生及其死亡后50年，截止于自然人死亡后第50年的12月31日；软件是合作开发的，截止于最后死亡的自然人死亡后第50年的12月31日。\\n\\xa0\\xa0\\xa0 法人或者其他组织的软件著作权，保护期为50年，截止于软件首次发表后第50年的12月31日，但软件自开发完成之日起50年内未发表的，本条例不再保护。', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='（二）文档，是指用来描述程序的内容、组成、设计、功能规格、开发情况、测试结果及使用方法的文字资料和图表等，如程序设计说明书、流程图、用户手册等。\\n\\xa0\\xa0\\xa0 （三）软件开发者，是指实际组织开发、直接进行开发，并对开发完成的软件承担责任的法人或者其他组织；或者依靠自己具有的条件独立完成软件开发，并对软件承担责任的自然人。\\n\\xa0\\xa0\\xa0 （四）软件著作权人，是指依照本条例的规定，对软件享有著作权的自然人、法人或者其他组织。\\n\\xa0\\xa0\\xa0 第四条 受本条例保护的软件必须由开发者独立开发，并已固定在某种有形物体上。\\n\\xa0\\xa0\\xa0 第五条 中国公民、法人或者其他组织对其所开发的软件，不论是否发表，依照本条例享有著作权。\\n\\xa0\\xa0\\xa0 外国人、无国籍人的软件首先在中国境内发行的，依照本条例享有著作权。\\n\\xa0\\xa0\\xa0 外国人、无国籍人的软件，依照其开发者所属国或者经常居住地国同中国签订的协议或者依照中国参加的国际条约享有的著作权，受本条例保护。\\n\\xa0\\xa0\\xa0 第六条 本条例对软件著作权的保护不延及开发软件所用的思想、处理过程、操作方法或者数学概念等。\\n\\xa0\\xa0\\xa0 第七条 软件著作权人可以向国务院著作权行政管理部门认定的软件登记机构办理登记。软件登记机构发放的登记证明文件是登记事项的初步证明。\\n\\xa0\\xa0\\xa0 办理软件登记应当缴纳费用。软件登记的收费标准由国务院著作权行政管理部门会同国务院价格主管部门规定。\\n第二章 软件著作权\\xa0\\n\\xa0\\xa0\\xa0\\xa0第八条 软件著作权人享有下列各项权利：\\n\\xa0\\xa0\\xa0 （一）发表权，即决定软件是否公之于众的权利；\\n\\xa0\\xa0\\xa0 （二）署名权，即表明开发者身份，在软件上署名的权利；\\n\\xa0\\xa0\\xa0 （三）修改权，即对软件进行增补、删节，或者改变指令、语句顺序的权利；\\n\\xa0\\xa0\\xa0 （四）复制权，即将软件制作一份或者多份的权利；\\n\\xa0\\xa0\\xa0 （五）发行权，即以出售或者赠与方式向公众提供软件的原件或者复制件的权利；\\n\\xa0\\xa0\\xa0 （六）出租权，即有偿许可他人临时使用软件的权利，但是软件不是出租的主要标的的除外；\\n\\xa0\\xa0\\xa0 （七）信息网络传播权，即以有线或者无线方式向公众提供软件，使公众可以在其个人选定的时间和地点获得软件的权利；\\n\\xa0\\xa0\\xa0 （八）翻译权，即将原软件从一种自然语言文字转换成另一种自然语言文字的权利；\\n\\xa0\\xa0\\xa0 （九）应当由软件著作权人享有的其他权利。\\n\\xa0\\xa0\\xa0\\xa0软件著作权人可以许可他人行使其软件著作权，并有权获得报酬。', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='第二十二条 中国公民、法人或者其他组织向外国人许可或者转让软件著作权的，应当遵守《中华人民共和国技术进出口管理条例》的有关规定。\\n第四章 法律责任\\xa0\\n\\xa0\\xa0\\xa0\\xa0第二十三条 除《中华人民共和国著作权法》或者本条例另有规定外，有下列侵权行为的，应当根据情况，承担停止侵害、消除影响、赔礼道歉、赔偿损失等民事责任：\\n\\xa0\\xa0\\xa0 （一）未经软件著作权人许可，发表或者登记其软件的；\\n\\xa0\\xa0\\xa0 （二）将他人软件作为自己的软件发表或者登记的；\\n\\xa0\\xa0\\xa0 （三）未经合作者许可，将与他人合作开发的软件作为自己单独完成的软件发表或者登记的；\\n\\xa0\\xa0\\xa0 （四）在他人软件上署名或者更改他人软件上的署名的；\\n\\xa0\\xa0\\xa0 （五）未经软件著作权人许可，修改、翻译其软件的；\\n\\xa0\\xa0\\xa0 （六）其他侵犯软件著作权的行为。\\n\\xa0\\xa0\\xa0 第二十四条 除《中华人民共和国著作权法》、本条例或者其他法律、行政法规另有规定外，未经软件著作权人许可，有下列侵权行为的，应当根据情况，承担停止侵害、消除影响、赔礼道歉、赔偿损失等民事责任；同时损害社会公共利益的，由著作权行政管理部门责令停止侵权行为，没收违法所得，没收、销毁侵权复制品，可以并处罚款；情节严重的，著作权行政管理部门并可以没收主要用于制作侵权复制品的材料、工具、设备等；触犯刑律的，依照刑法关于侵犯著作权罪、销售侵权复制品罪的规定，依法追究刑事责任：\\n\\xa0\\xa0\\xa0 （一）复制或者部分复制著作权人的软件的；\\n\\xa0\\xa0\\xa0 （二）向公众发行、出租、通过信息网络传播著作权人的软件的；\\n\\xa0\\xa0\\xa0 （三）故意避开或者破坏著作权人为保护其软件著作权而采取的技术措施的；\\n\\xa0\\xa0\\xa0 （四）故意删除或者改变软件权利管理电子信息的；\\n\\xa0\\xa0\\xa0 （五）转让或者许可他人行使著作权人的软件著作权的。\\n\\xa0\\xa0\\xa0 有前款第一项或者第二项行为的，可以并处每件100元或者货值金额1倍以上５倍以下的罚款；有前款第三项、第四项或者第五项行为的，可以并处20万元以下的罚款。\\n\\xa0\\xa0\\xa0 第二十五条 侵犯软件著作权的赔偿数额，依照《中华人民共和国著作权法》第四十九条的规定确定。\\n\\xa0\\xa0\\xa0 第二十六条 软件著作权人有证据证明他人正在实施或者即将实施侵犯其权利的行为，如不及时制止，将会使其合法权益受到难以弥补的损害的，可以依照《中华人民共和国著作权法》第五十条的规定，在提起诉讼前向人民法院申请采取责令停止有关行为和财产保全的措施。', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='第二十七条 为了制止侵权行为，在证据可能灭失或者以后难以取得的情况下，软件著作权人可以依照《中华人民共和国著作权法》第五十一条的规定，在提起诉讼前向人民法院申请保全证据。\\n\\xa0\\xa0\\xa0 第二十八条 软件复制品的出版者、制作者不能证明其出版、制作有合法授权的，或者软件复制品的发行者、出租者不能证明其发行、出租的复制品有合法来源的，应当承担法律责任。\\n\\xa0\\xa0\\xa0 第二十九条 软件开发者开发的软件，由于可供选用的表达方式有限而与已经存在的软件相似的，不构成对已经存在的软件的著作权的侵犯。\\n\\xa0\\xa0\\xa0 第三十条 软件的复制品持有人不知道也没有合理理由应当知道该软件是侵权复制品的，不承担赔偿责任；但是，应当停止使用、销毁该侵权复制品。如果停止使用并销毁该侵权复制品将给复制品使用人造成重大损失的，复制品使用人可以在向软件著作权人支付合理费用后继续使用。\\n\\xa0\\xa0\\xa0 第三十一条 软件著作权侵权纠纷可以调解。\\n\\xa0\\xa0\\xa0 软件著作权合同纠纷可以依据合同中的仲裁条款或者事后达成的书面仲裁协议，向仲裁机构申请仲裁。\\n\\xa0\\xa0\\xa0 当事人没有在合同中订立仲裁条款，事后又没有书面仲裁协议的，可以直接向人民法院提起诉讼。\\n第五章 附\\xa0 则\\xa0\\n\\xa0\\xa0\\xa0\\xa0第三十二条 本条例施行前发生的侵权行为，依照侵权行为发生时的国家有关规定处理。\\n\\xa0\\xa0\\xa0 第三十三条 本条例自2002年1月1日起施行。1991年6月4日国务院发布的《计算机软件保护条例》同时废止。', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='中华人民共和国国务院令\\n第632号\\n\\xa0\\xa0\\xa0\\xa0\\xa0《国务院关于修改〈计算机软件保护条例〉的决定》已经2013年1月16日国务院第231次常务会议通过，现予公布，自2013年3月1日起施行。\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 总理\\xa0\\xa0 温家宝\\n\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000\\u3000 \\xa0\\u30002013年1月30日\\n国务院关于修改《计算机软件保护条例》的决定\\n\\xa0\\xa0\\xa0 国务院决定对《计算机软件保护条例》作如下修改：\\n\\xa0\\xa0\\xa0 将第二十四条第二款修改为：“有前款第一项或者第二项行为的，可以并处每件100元或者货值金额1倍以上5倍以下的罚款；有前款第三项、第四项或者第五项行为的，可以并处20万元以下的罚款。”\\n\\xa0\\xa0\\xa0 本决定自2013年3月1日起施行。《计算机软件保护条例》根据本决定作相应修改，重新公布。\\n\\xa0\\n计算机软件保护条例\\n\\xa0\\xa0 （2001年12月20日中华人民共和国国务院令第339号公布\\u3000根据2011年1月8日《国务院关于废止和修改部分行政法规的决定》第一次修订\\u3000根据2013年1月30日《国务院关于修改〈计算机软件保护条例〉的决定》第二次修订）\\n第一章 总 则\\n\\xa0\\xa0\\xa0\\xa0第一条 为了保护计算机软件著作权人的权益，调整计算机软件在开发、传播和使用中发生的利益关系，鼓励计算机软件的开发与应用，促进软件产业和国民经济信息化的发展，根据《中华人民共和国著作权法》，制定本条例。\\n\\xa0\\xa0\\xa0 第二条 本条例所称计算机软件（以下简称软件），是指计算机程序及其有关文档。\\n\\xa0\\xa0\\xa0 第三条 本条例下列用语的含义：\\n\\xa0\\xa0\\xa0 （一）计算机程序，是指为了得到某种结果而可以由计算机等具有信息处理能力的装置执行的代码化指令序列，或者可以被自动转换成代码化指令序列的符号化指令序列或者符号化语句序列。同一计算机程序的源程序和目标程序为同一作品。\\n\\xa0\\xa0\\xa0 （二）文档，是指用来描述程序的内容、组成、设计、功能规格、开发情况、测试结果及使用方法的文字资料和图表等，如程序设计说明书、流程图、用户手册等。', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "5872475f30b2ad7a",
   "metadata": {},
   "source": [
    "上面已经实现了从文档数据分片，然后向量化，然后存放到向量库，最后通过问题相识度搜索召回相关文档片段的过程了。这部分基本就是RAG的离线部分。在完成这些之后，用户就会询问问题，然后将问题也向量化，检索出相关文档，将检索出来的文档给模型然后回答问题，就是RAG的在线部分。而这部分就需要用到Retriever了。\n",
    "#### Retriever\n",
    "前面已经通过相似度搜索召回了k个和问题相关的片段了，但是真实的情况远不止这么简单。我们要考虑如何召回相关度最高的k个文档，又或者召回和问题关联度超过一定值的文档。召回的文档如果太多了，全部给模型可能会导致超过窗口大小，那么又如何处理。langchain内置了非常多的Retriever用于解决数据召回，我们这次因为只是总览一下langchain，只介绍其中最简单和常用的一个VectorStoreRetriever，其他可以查阅[文档](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1210cc693408058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T05:00:16.164282Z",
     "start_time": "2024-06-26T05:00:12.152272Z"
    }
   },
   "source": [
    "# RAGOnline.ts\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = 'share-langchain-rag'\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model='text-embedding-v2',\n",
    "    dashscope_api_key='sk-90cba3edc3e1412b84547915475dca30'\n",
    ")\n",
    "vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "# retriever = vector_store.as_retriever(search_type='similarity_score_threshold',search_kwargs={'k': 3, 'score_threshold': 0.8})\n",
    "# retriever = vector_store.as_retriever(search_type='mmr',search_kwargs={'k': 3})\n",
    "retriever.invoke(\"根据我国著作权保护法，软件保护期是多久\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='法人或者其他组织的软件著作权，保护期为50年，截止于软件首次发表后第50年的12月31日，但软件自开发完成之日起50年内未发表的，本条例不再保护。\\n\\xa0\\xa0\\xa0 第十五条 软件著作权属于自然人的，该自然人死亡后，在软件著作权的保护期内，软件著作权的继承人可以依照《中华人民共和国继承法》的有关规定，继承本条例第八条规定的除署名权以外的其他权利。\\n\\xa0\\xa0\\xa0\\xa0软件著作权属于法人或者其他组织的，法人或者其他组织变更、终止后，其著作权在本条例规定的保护期内由承受其权利义务的法人或者其他组织享有；没有承受其权利义务的法人或者其他组织的，由国家享有。\\n\\xa0\\xa0\\xa0 第十六条 软件的合法复制品所有人享有下列权利：\\n\\xa0\\xa0\\xa0 （一）根据使用的需要把该软件装入计算机等具有信息处理能力的装置内；\\n\\xa0\\xa0\\xa0 （二）为了防止复制品损坏而制作备份复制品。这些备份复制品不得通过任何方式提供给他人使用，并在所有人丧失该合法复制品的所有权时，负责将备份复制品销毁；\\n\\xa0\\xa0\\xa0 （三）为了把该软件用于实际的计算机应用环境或者改进其功能、性能而进行必要的修改；但是，除合同另有约定外，未经该软件著作权人许可，不得向任何第三方提供修改后的软件。\\n\\xa0\\xa0\\xa0 第十七条 为了学习和研究软件内含的设计思想和原理，通过安装、显示、传输或者存储软件等方式使用软件的，可以不经软件著作权人许可，不向其支付报酬。\\n第三章 软件著作权的许可使用和转让\\xa0\\n\\xa0\\xa0\\xa0\\xa0第十八条 许可他人行使软件著作权的，应当订立许可使用合同。\\n\\xa0\\xa0\\xa0 许可使用合同中软件著作权人未明确许可的权利，被许可人不得行使。\\n\\xa0\\xa0\\xa0 第十九条 许可他人专有行使软件著作权的，当事人应当订立书面合同。\\n\\xa0\\xa0\\xa0 没有订立书面合同或者合同中未明确约定为专有许可的，被许可行使的权利应当视为非专有权利。\\n\\xa0\\xa0\\xa0 第二十条 转让软件著作权的，当事人应当订立书面合同。\\n\\xa0\\xa0\\xa0 第二十一条 订立许可他人专有行使软件著作权的许可合同，或者订立转让软件著作权合同，可以向国务院著作权行政管理部门认定的软件登记机构登记。\\n\\xa0\\xa0\\xa0 第二十二条 中国公民、法人或者其他组织向外国人许可或者转让软件著作权的，应当遵守《中华人民共和国技术进出口管理条例》的有关规定。\\n第四章 法律责任', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='法人或者其他组织的软件著作权，保护期为50年，截止于软件首次发表后第50年的12月31日，但软件自开发完成之日起50年内未发表的，本条例不再保护。\\n\\xa0\\xa0\\xa0 第十五条 软件著作权属于自然人的，该自然人死亡后，在软件著作权的保护期内，软件著作权的继承人可以依照《中华人民共和国继承法》的有关规定，继承本条例第八条规定的除署名权以外的其他权利。\\n\\xa0\\xa0\\xa0\\xa0软件著作权属于法人或者其他组织的，法人或者其他组织变更、终止后，其著作权在本条例规定的保护期内由承受其权利义务的法人或者其他组织享有；没有承受其权利义务的法人或者其他组织的，由国家享有。\\n\\xa0\\xa0\\xa0 第十六条 软件的合法复制品所有人享有下列权利：\\n\\xa0\\xa0\\xa0 （一）根据使用的需要把该软件装入计算机等具有信息处理能力的装置内；\\n\\xa0\\xa0\\xa0 （二）为了防止复制品损坏而制作备份复制品。这些备份复制品不得通过任何方式提供给他人使用，并在所有人丧失该合法复制品的所有权时，负责将备份复制品销毁；\\n\\xa0\\xa0\\xa0 （三）为了把该软件用于实际的计算机应用环境或者改进其功能、性能而进行必要的修改；但是，除合同另有约定外，未经该软件著作权人许可，不得向任何第三方提供修改后的软件。\\n\\xa0\\xa0\\xa0 第十七条 为了学习和研究软件内含的设计思想和原理，通过安装、显示、传输或者存储软件等方式使用软件的，可以不经软件著作权人许可，不向其支付报酬。\\n第三章 软件著作权的许可使用和转让\\xa0\\n\\xa0\\xa0\\xa0\\xa0第十八条 许可他人行使软件著作权的，应当订立许可使用合同。\\n\\xa0\\xa0\\xa0 许可使用合同中软件著作权人未明确许可的权利，被许可人不得行使。\\n\\xa0\\xa0\\xa0 第十九条 许可他人专有行使软件著作权的，当事人应当订立书面合同。\\n\\xa0\\xa0\\xa0 没有订立书面合同或者合同中未明确约定为专有许可的，被许可行使的权利应当视为非专有权利。\\n\\xa0\\xa0\\xa0 第二十条 转让软件著作权的，当事人应当订立书面合同。\\n\\xa0\\xa0\\xa0 第二十一条 订立许可他人专有行使软件著作权的许可合同，或者订立转让软件著作权合同，可以向国务院著作权行政管理部门认定的软件登记机构登记。\\n\\xa0\\xa0\\xa0 第二十二条 中国公民、法人或者其他组织向外国人许可或者转让软件著作权的，应当遵守《中华人民共和国技术进出口管理条例》的有关规定。\\n第四章 法律责任', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'}),\n",
       " Document(page_content='（九）应当由软件著作权人享有的其他权利。\\n\\xa0\\xa0\\xa0\\xa0软件著作权人可以许可他人行使其软件著作权，并有权获得报酬。\\n\\xa0\\xa0\\xa0 软件著作权人可以全部或者部分转让其软件著作权，并有权获得报酬。\\n\\xa0\\xa0\\xa0 第九条 软件著作权属于软件开发者，本条例另有规定的除外。\\n\\xa0\\xa0\\xa0 如无相反证明，在软件上署名的自然人、法人或者其他组织为开发者。\\n\\xa0\\xa0\\xa0 第十条 由两个以上的自然人、法人或者其他组织合作开发的软件，其著作权的归属由合作开发者签订书面合同约定。无书面合同或者合同未作明确约定，合作开发的软件可以分割使用的，开发者对各自开发的部分可以单独享有著作权；但是，行使著作权时，不得扩展到合作开发的软件整体的著作权。合作开发的软件不能分割使用的，其著作权由各合作开发者共同享有，通过协商一致行使；不能协商一致，又无正当理由的，任何一方不得阻止他方行使除转让权以外的其他权利，但是所得收益应当合理分配给所有合作开发者。\\n\\xa0\\xa0\\xa0 第十一条 接受他人委托开发的软件，其著作权的归属由委托人与受托人签订书面合同约定；无书面合同或者合同未作明确约定的，其著作权由受托人享有。\\n\\xa0\\xa0\\xa0 第十二条 由国家机关下达任务开发的软件，著作权的归属与行使由项目任务书或者合同规定；项目任务书或者合同中未作明确规定的，软件著作权由接受任务的法人或者其他组织享有。\\n\\xa0\\xa0\\xa0 第十三条 自然人在法人或者其他组织中任职期间所开发的软件有下列情形之一的，该软件著作权由该法人或者其他组织享有，该法人或者其他组织可以对开发软件的自然人进行奖励：\\n\\xa0\\xa0\\xa0 （一）针对本职工作中明确指定的开发目标所开发的软件；\\n\\xa0\\xa0\\xa0 （二）开发的软件是从事本职工作活动所预见的结果或者自然的结果；\\n\\xa0\\xa0\\xa0 （三）主要使用了法人或者其他组织的资金、专用设备、未公开的专门信息等物质技术条件所开发并由法人或者其他组织承担责任的软件。\\n\\xa0\\xa0\\xa0 第十四条 软件著作权自软件开发完成之日起产生。\\n\\xa0\\xa0\\xa0 自然人的软件著作权，保护期为自然人终生及其死亡后50年，截止于自然人死亡后第50年的12月31日；软件是合作开发的，截止于最后死亡的自然人死亡后第50年的12月31日。\\n\\xa0\\xa0\\xa0 法人或者其他组织的软件著作权，保护期为50年，截止于软件首次发表后第50年的12月31日，但软件自开发完成之日起50年内未发表的，本条例不再保护。', metadata={'source': 'https://www.gov.cn/flfg/2013-02/08/content_2332395.htm'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "81af0b9a2f456a3b",
   "metadata": {},
   "source": [
    "现在我们基本上完成了RAG需要的所有步骤了，我们实现一般会将离线和在线部分分离开，因为离线部分主要是完成数据的准备，用户侧不会调用，而且一般也是非实时的。而在线部分基本都是用户的访问插叙，一般也要求延迟不要太低。我们将目前的在线部分全部实现出来就是："
   ]
  },
  {
   "cell_type": "code",
   "id": "b4d4c159740140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T05:03:46.577415Z",
     "start_time": "2024-06-26T05:00:16.173955Z"
    }
   },
   "source": [
    "# CompleteDemoRAG.ts\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from libs.llm.ollama import ollama\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if os.environ.get('PINECONE_API_KEY') is None:\n",
    "    os.environ['PINECONE_API_KEY'] = getpass.getpass('PINECONE_API_KEY: ')\n",
    "\n",
    "template = \"\"\"\n",
    "你是一个用于问答任务的助手。\n",
    "使用以下检索到的内容来回答问题。\n",
    "如果你不知道答案，就说你不知道。\n",
    "保持答案的简洁\n",
    "问题: \n",
    "-------\n",
    "{question}\n",
    "------- \n",
    "\n",
    "上下文: \n",
    "-------\n",
    "{context} \n",
    "-------\n",
    "回答:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "index_name = 'share-langchain-rag'\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model='text-embedding-v2',\n",
    "    dashscope_api_key='sk-90cba3edc3e1412b84547915475dca30'\n",
    ")\n",
    "vector_store = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_type='similarity_score_threshold',\n",
    "                                      search_kwargs={'k': 3, 'score_threshold': 0.7})\n",
    "rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | ollama\n",
    "        | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"根据我国著作权保护法，软件保护期是多久\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据上述法律条文，自然人的软件著作权保护期为自生或者终生以来的50年，到50年之日。此外，如果软件是合作开发的，则由各合作开发人单独享有对他们开发贡献的权利；而若无书面合同约定，则可能出现分割使用的情形。法人或其他组织的著作权保护期为50年，但如果软件未在开发完成之日以来发表，则本条适用不再。\\n\\n自然人在法人或其他组织中任职期间的奖励权利包�ited有：\\n\\n1. 对于明确指定的开发目标所开发的软件，\\n\\n2. 该软件是从事本职工作活动预见的结果或自然的结果，\\n\\n3. 主要使用了法人或其他组织的资金、专用设备或未公开的专门信息等物质技术条件。\\n\\n此外，软件著作权自发现之日起保护期为自生以来50年，直到50年之日。'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "490cffde-4214-4c77-97bf-7cca5a58915a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T05:03:46.592836Z",
     "start_time": "2024-06-26T05:03:46.583256Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
